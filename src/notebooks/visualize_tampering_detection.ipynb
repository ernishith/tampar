{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Tampering Detection Analysis\n",
    "\n",
    "This notebook visualizes **why** a parcel is classified as tampered by showing:\n",
    "1. Ground truth UV map patches\n",
    "2. Detected image patches\n",
    "3. Preprocessed patches (after applying compare methods)\n",
    "4. Similarity scores for each surface\n",
    "5. Decision tree classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths\n",
    "ROOT = Path.cwd().parent.parent  # Adjust if needed\n",
    "sys.path.append(ROOT.as_posix())\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "from src.tampering.compare import CompareType, apply_homogenization, METRICS\n",
    "from src.tampering.parcel import PATCH_ORDER\n",
    "from src.tampering.utils import get_side_surface_patches\n",
    "from src.tampering.metrics import compute_msssim, compute_ssim, compute_hog, compute_mae\n",
    "\n",
    "# Load similarity scores\n",
    "df = pd.read_csv(ROOT / 'data' / 'misc' / 'simscores_validation.csv')\n",
    "print(f\"Loaded {len(df)} similarity score records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Select a Parcel to Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available parcels and their tampering status\n",
    "parcel_summary = df.groupby('parcel_id').agg({\n",
    "    'tampered': 'first',\n",
    "    'tampering': 'first',\n",
    "    'view': 'nunique'\n",
    "}).reset_index()\n",
    "parcel_summary.columns = ['parcel_id', 'is_tampered', 'tampering_type', 'num_views']\n",
    "print(\"\\nAvailable parcels:\")\n",
    "print(parcel_summary)\n",
    "\n",
    "# Filter tampered parcels\n",
    "tampered_parcels = parcel_summary[parcel_summary['is_tampered'] == True]\n",
    "print(f\"\\nFound {len(tampered_parcels)} tampered parcels\")\n",
    "print(tampered_parcels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT YOUR PARCEL HERE\n",
    "PARCEL_ID = 2  # Change this to any parcel ID you want to analyze\n",
    "\n",
    "# Get views for this parcel (excluding ground truth uvmaps)\n",
    "parcel_views = df[(df['parcel_id'] == PARCEL_ID) & (df['gt_keypoints'] == False)]['view'].unique()\n",
    "print(f\"\\nParcel {PARCEL_ID} has {len(parcel_views)} views\")\n",
    "\n",
    "# Select first view for analysis\n",
    "if len(parcel_views) > 0:\n",
    "    selected_view = parcel_views[0]\n",
    "    print(f\"Analyzing view: {selected_view}\")\n",
    "    \n",
    "    # Show tampering info\n",
    "    view_data = df[(df['parcel_id'] == PARCEL_ID) & (df['view'] == selected_view)]\n",
    "    print(f\"\\nTampering status: {view_data['tampered'].iloc[0]}\")\n",
    "    print(f\"Tampering type: {view_data['tampering'].iloc[0]}\")\n",
    "else:\n",
    "    print(\"No views found for this parcel!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Display UV Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_ROOT = ROOT / \"data\" / \"tampar_sample\"\n",
    "\n",
    "# Load ground truth UV map\n",
    "gt_uvmap_path = IMAGE_ROOT / \"uvmaps\" / f\"id_{str(PARCEL_ID).zfill(2)}_uvmap.png\"\n",
    "gt_uvmap = cv2.imread(gt_uvmap_path.as_posix())\n",
    "gt_uvmap = cv2.cvtColor(gt_uvmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Load detected UV map (from the selected view)\n",
    "detected_uvmap_path = IMAGE_ROOT / selected_view\n",
    "detected_uvmap = cv2.imread(detected_uvmap_path.as_posix())\n",
    "detected_uvmap = cv2.cvtColor(detected_uvmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display both UV maps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "axes[0].imshow(gt_uvmap)\n",
    "axes[0].set_title('Ground Truth UV Map\\n(Reference)', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(detected_uvmap)\n",
    "axes[1].set_title('Detected UV Map\\n(Test Image)', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ground truth shape: {gt_uvmap.shape}\")\n",
    "print(f\"Detected shape: {detected_uvmap.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract and Compare Individual Surface Patches\n",
    "\n",
    "This shows each surface (top, left, center, right, bottom) side-by-side with similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patches from both UV maps\n",
    "gt_patches = list(get_side_surface_patches(gt_uvmap))\n",
    "detected_patches = list(get_side_surface_patches(detected_uvmap))\n",
    "\n",
    "# Surface names (matching PATCH_ORDER)\n",
    "surface_names = ['top', 'left', 'center', 'right', 'bottom']\n",
    "\n",
    "print(f\"Extracted {len(gt_patches)} surface patches\")\n",
    "print(f\"Patch size: {gt_patches[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Each Surface with Multiple Compare Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_surface_comparison(surface_idx, surface_name, compare_type='plain'):\n",
    "    \"\"\"\n",
    "    Visualize comparison for a single surface\n",
    "    Shows: [GT patch] [Detected patch] [GT processed] [Detected processed] + scores\n",
    "    \"\"\"\n",
    "    gt_patch = gt_patches[surface_idx]\n",
    "    detected_patch = detected_patches[surface_idx]\n",
    "    \n",
    "    # Skip white patches\n",
    "    if np.mean(gt_patch) >= 250 or np.mean(detected_patch) >= 250:\n",
    "        print(f\"Skipping {surface_name} - white patch\")\n",
    "        return\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    processed_gt, processed_det = apply_homogenization(\n",
    "        compare_type, gt_patch, detected_patch\n",
    "    )\n",
    "    \n",
    "    # Compute metrics\n",
    "    msssim_score = compute_msssim(processed_gt.astype(np.float32), processed_det.astype(np.float32))\n",
    "    ssim_score = compute_ssim(processed_gt.astype(np.float32), processed_det.astype(np.float32))\n",
    "    hog_score = compute_hog(processed_gt.astype(np.float32), processed_det.astype(np.float32))\n",
    "    mae_score = compute_mae(processed_gt.astype(np.float32), processed_det.astype(np.float32))\n",
    "    \n",
    "    # Get ground truth label for this surface\n",
    "    surface_data = df[\n",
    "        (df['parcel_id'] == PARCEL_ID) & \n",
    "        (df['view'] == selected_view) & \n",
    "        (df['sideface_name'] == surface_name) &\n",
    "        (df['compare_type'] == compare_type)\n",
    "    ]\n",
    "    \n",
    "    is_tampered = surface_data['tampered'].iloc[0] if len(surface_data) > 0 else False\n",
    "    tampering_type = surface_data['tampering'].iloc[0] if len(surface_data) > 0 else ''\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "    \n",
    "    # Original patches\n",
    "    axes[0, 0].imshow(gt_patch.astype(int))\n",
    "    axes[0, 0].set_title('Ground Truth\\n(Reference)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(detected_patch.astype(int))\n",
    "    axes[0, 1].set_title('Detected\\n(Test)', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Processed patches\n",
    "    axes[1, 0].imshow(processed_gt.astype(int))\n",
    "    axes[1, 0].set_title(f'After {compare_type}\\n(Reference)', fontsize=12)\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(processed_det.astype(int))\n",
    "    axes[1, 1].set_title(f'After {compare_type}\\n(Test)', fontsize=12)\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # Add title with scores\n",
    "    title_color = 'red' if is_tampered else 'green'\n",
    "    tampered_text = f\"TAMPERED ({tampering_type})\" if is_tampered else \"NOT TAMPERED\"\n",
    "    \n",
    "    fig.suptitle(\n",
    "        f'Surface: {surface_name.upper()} - {tampered_text}\\n'\n",
    "        f'Compare Method: {compare_type}\\n'\n",
    "        f'MSSSIM: {msssim_score:.4f} | SSIM: {ssim_score:.4f} | HOG: {hog_score:.4f} | MAE: {mae_score:.4f}\\n'\n",
    "        f'(Higher SSIM/MSSSIM = more similar | Lower HOG/MAE = more similar)',\n",
    "        fontsize=14,\n",
    "        fontweight='bold',\n",
    "        color=title_color\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'msssim': msssim_score,\n",
    "        'ssim': ssim_score,\n",
    "        'hog': hog_score,\n",
    "        'mae': mae_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare All Surfaces with 'plain' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize each surface\n",
    "for i, surface_name in enumerate(surface_names):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing surface: {surface_name.upper()}\")\n",
    "    print('='*60)\n",
    "    visualize_surface_comparison(i, surface_name, compare_type='plain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Different Compare Methods on a Specific Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a surface to analyze with different methods\n",
    "SURFACE_TO_ANALYZE = 'center'  # Change to 'top', 'left', 'right', 'bottom'\n",
    "surface_idx = surface_names.index(SURFACE_TO_ANALYZE)\n",
    "\n",
    "# Try all compare methods\n",
    "compare_methods = ['plain', 'canny', 'laplacian', 'meanchannel']\n",
    "\n",
    "for method in compare_methods:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Compare method: {method.upper()}\")\n",
    "    print('='*60)\n",
    "    visualize_surface_comparison(surface_idx, SURFACE_TO_ANALYZE, compare_type=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Summary Heatmap - All Surfaces, All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all scores for this view\n",
    "view_scores = df[(df['parcel_id'] == PARCEL_ID) & (df['view'] == selected_view)]\n",
    "\n",
    "# Create heatmaps for each metric\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(f'Similarity Scores Heatmap - Parcel {PARCEL_ID}', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics = ['msssim', 'ssim', 'hog', 'mae']\n",
    "titles = [\n",
    "    'MSSSIM (higher=similar)',\n",
    "    'SSIM (higher=similar)',\n",
    "    'HOG Distance (lower=similar)',\n",
    "    'MAE (lower=similar)'\n",
    "]\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Pivot data\n",
    "    pivot = view_scores.pivot_table(\n",
    "        index='sideface_name',\n",
    "        columns='compare_type',\n",
    "        values=metric,\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Plot heatmap\n",
    "    im = ax.imshow(pivot, cmap='RdYlGn' if metric in ['msssim', 'ssim'] else 'RdYlGn_r', aspect='auto')\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xticks(np.arange(len(pivot.columns)))\n",
    "    ax.set_yticks(np.arange(len(pivot.index)))\n",
    "    ax.set_xticklabels(pivot.columns, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Add values\n",
    "    for i in range(len(pivot.index)):\n",
    "        for j in range(len(pivot.columns)):\n",
    "            value = pivot.iloc[i, j]\n",
    "            if not np.isnan(value):\n",
    "                ax.text(j, i, f'{value:.3f}', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Identify Most Different Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average dissimilarity for each surface\n",
    "surface_scores = view_scores.groupby('sideface_name').agg({\n",
    "    'msssim': 'mean',\n",
    "    'ssim': 'mean',\n",
    "    'hog': 'mean',\n",
    "    'mae': 'mean',\n",
    "    'tampered': 'first',\n",
    "    'tampering': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Sort by MSSSIM (lower = more different)\n",
    "surface_scores_sorted = surface_scores.sort_values('msssim')\n",
    "\n",
    "print(\"\\nSurface Similarity Ranking (least similar first):\")\n",
    "print(\"=\"*80)\n",
    "for _, row in surface_scores_sorted.iterrows():\n",
    "    tampered_indicator = \"ðŸ”´ TAMPERED\" if row['tampered'] else \"ðŸŸ¢ OK\"\n",
    "    print(f\"{row['sideface_name']:10s} | MSSSIM: {row['msssim']:.4f} | HOG: {row['hog']:.4f} | {tampered_indicator}\")\n",
    "    if row['tampered']:\n",
    "        print(f\"           | Tampering type: {row['tampering']}\")\n",
    "\n",
    "most_different = surface_scores_sorted.iloc[0]\n",
    "print(f\"\\nâš ï¸  Most different surface: {most_different['sideface_name'].upper()}\")\n",
    "print(f\"   MSSSIM: {most_different['msssim']:.4f}\")\n",
    "print(f\"   Actually tampered: {most_different['tampered']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize the Most Different Surface in Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_diff_surface = most_different['sideface_name']\n",
    "most_diff_idx = surface_names.index(most_diff_surface)\n",
    "\n",
    "print(f\"Detailed analysis of most different surface: {most_diff_surface.upper()}\")\n",
    "print(\"\\nTrying all compare methods:\")\n",
    "\n",
    "for method in ['plain', 'canny', 'laplacian', 'meanchannel']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Method: {method}\")\n",
    "    print('='*60)\n",
    "    visualize_surface_comparison(most_diff_idx, most_diff_surface, compare_type=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare True Positive vs False Positive (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a true positive and false positive example\n",
    "tp_views = df[(df['tampered'] == True) & (df['gt_keypoints'] == False)]['view'].unique()\n",
    "fp_views = df[(df['tampered'] == False) & (df['gt_keypoints'] == False)]['view'].unique()\n",
    "\n",
    "print(f\"True positive views: {len(tp_views)}\")\n",
    "print(f\"False positive views: {len(fp_views)}\")\n",
    "\n",
    "if len(tp_views) > 0 and len(fp_views) > 0:\n",
    "    # Get average scores\n",
    "    tp_scores = df[df['view'].isin(tp_views)].groupby('sideface_name')[['msssim', 'hog']].mean()\n",
    "    fp_scores = df[df['view'].isin(fp_views)].groupby('sideface_name')[['msssim', 'hog']].mean()\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    surfaces = tp_scores.index\n",
    "    x = np.arange(len(surfaces))\n",
    "    width = 0.35\n",
    "    \n",
    "    # MSSSIM comparison\n",
    "    axes[0].bar(x - width/2, tp_scores['msssim'], width, label='True Positive (Tampered)', color='red', alpha=0.7)\n",
    "    axes[0].bar(x + width/2, fp_scores['msssim'], width, label='False Positive (Not Tampered)', color='orange', alpha=0.7)\n",
    "    axes[0].set_xlabel('Surface')\n",
    "    axes[0].set_ylabel('MSSSIM Score')\n",
    "    axes[0].set_title('MSSSIM: True Positive vs False Positive\\n(lower = more different)', fontweight='bold')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(surfaces)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # HOG comparison\n",
    "    axes[1].bar(x - width/2, tp_scores['hog'], width, label='True Positive (Tampered)', color='red', alpha=0.7)\n",
    "    axes[1].bar(x + width/2, fp_scores['hog'], width, label='False Positive (Not Tampered)', color='orange', alpha=0.7)\n",
    "    axes[1].set_xlabel('Surface')\n",
    "    axes[1].set_ylabel('HOG Distance')\n",
    "    axes[1].set_title('HOG: True Positive vs False Positive\\n(higher = more different)', fontweight='bold')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(surfaces)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- True Positives: Should show LOW MSSSIM (< 0.5) and HIGH HOG (> 10) on tampered surfaces\")\n",
    "    print(\"- False Positives: Show moderate scores due to lighting/angle differences, not actual tampering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook shows you:\n",
    "1. **What patches are being compared** - Ground truth vs detected UV map patches\n",
    "2. **How preprocessing affects comparison** - Different compare methods (plain, canny, laplacian, etc.)\n",
    "3. **Why a surface is flagged as tampered** - Visual differences + quantitative similarity scores\n",
    "4. **Which surface is most different** - Ranked by similarity metrics\n",
    "\n",
    "### Key Insights:\n",
    "- **Low MSSSIM/SSIM** (< 0.5) indicates visual differences\n",
    "- **High HOG/MAE** (> 10 for HOG) indicates texture/structure differences\n",
    "- **True tampering** shows consistent low scores across multiple compare methods\n",
    "- **False positives** often due to lighting, perspective, or acquisition differences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
